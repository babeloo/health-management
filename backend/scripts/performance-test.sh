#!/bin/bash

# InfluxDB æ€§èƒ½æµ‹è¯•è„šæœ¬
# ç›®æ ‡ï¼šéªŒè¯æŸ¥è¯¢å“åº”æ—¶é—´ < 100ms

set -e

INFLUX_URL="http://localhost:8086"
INFLUX_TOKEN="my-super-secret-auth-token"
INFLUX_ORG="vakyi"
INFLUX_BUCKET="health_data"
USER_ID="test_user_123"

echo "======================================"
echo "InfluxDB æ€§èƒ½æµ‹è¯•"
echo "======================================"

# 1. å†™å…¥æµ‹è¯•æ•°æ®ï¼ˆè¡€å‹ï¼‰
echo ""
echo "ğŸ“ å†™å…¥è¡€å‹æµ‹è¯•æ•°æ®..."
for i in {1..50}; do
  TIMESTAMP=$(date -u -d "$i days ago" +%s)
  SYSTOLIC=$((100 + RANDOM % 40))
  DIASTOLIC=$((60 + RANDOM % 30))
  PULSE=$((60 + RANDOM % 40))

  curl -s -XPOST "${INFLUX_URL}/api/v2/write?org=${INFLUX_ORG}&bucket=${INFLUX_BUCKET}&precision=s" \
    --header "Authorization: Token ${INFLUX_TOKEN}" \
    --data-raw "blood_pressure,user_id=${USER_ID},check_in_id=test_${i} systolic=${SYSTOLIC},diastolic=${DIASTOLIC},pulse=${PULSE} ${TIMESTAMP}" > /dev/null
done
echo "âœ… å·²å†™å…¥ 50 æ¡è¡€å‹æ•°æ®"

# 2. å†™å…¥æµ‹è¯•æ•°æ®ï¼ˆè¡€ç³–ï¼‰
echo ""
echo "ğŸ“ å†™å…¥è¡€ç³–æµ‹è¯•æ•°æ®..."
TIMINGS=("fasting" "postprandial" "random")
for i in {1..50}; do
  TIMESTAMP=$(date -u -d "$i days ago" +%s)
  TIMING=${TIMINGS[$((RANDOM % 3))]}
  VALUE=$(echo "4.5 + $RANDOM % 50 / 10" | bc -l | awk '{printf "%.1f", $0}')

  curl -s -XPOST "${INFLUX_URL}/api/v2/write?org=${INFLUX_ORG}&bucket=${INFLUX_BUCKET}&precision=s" \
    --header "Authorization: Token ${INFLUX_TOKEN}" \
    --data-raw "blood_sugar,user_id=${USER_ID},check_in_id=test_${i},timing=${TIMING} value=${VALUE} ${TIMESTAMP}" > /dev/null
done
echo "âœ… å·²å†™å…¥ 50 æ¡è¡€ç³–æ•°æ®"

# ç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ
sleep 2

# 3. æ€§èƒ½æµ‹è¯•ï¼šæŸ¥è¯¢æœ€è¿‘ 7 å¤©è¡€å‹è¶‹åŠ¿
echo ""
echo "âš¡ æ€§èƒ½æµ‹è¯• 1: æŸ¥è¯¢æœ€è¿‘ 7 å¤©è¡€å‹è¶‹åŠ¿ï¼ˆæŒ‰å¤©èšåˆï¼‰"
QUERY_1='from(bucket: "health_data")
  |> range(start: -7d, stop: now())
  |> filter(fn: (r) =>
      r._measurement == "blood_pressure" and
      r.user_id == "test_user_123"
  )
  |> aggregateWindow(every: 1d, fn: mean, createEmpty: false)
  |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
  |> keep(columns: ["_time", "systolic", "diastolic", "pulse"])
  |> sort(columns: ["_time"], desc: false)'

START=$(date +%s%N)
RESULT_1=$(curl -s -XPOST "${INFLUX_URL}/api/v2/query?org=${INFLUX_ORG}" \
  --header "Authorization: Token ${INFLUX_TOKEN}" \
  --header "Content-Type: application/vnd.flux" \
  --data "${QUERY_1}")
END=$(date +%s%N)
ELAPSED_MS=$(( (END - START) / 1000000 ))

echo "   å“åº”æ—¶é—´: ${ELAPSED_MS}ms"
if [ $ELAPSED_MS -lt 100 ]; then
  echo "   âœ… æ€§èƒ½è¾¾æ ‡ï¼ˆ< 100msï¼‰"
else
  echo "   âš ï¸  æ€§èƒ½æœªè¾¾æ ‡ï¼ˆç›®æ ‡ < 100msï¼‰"
fi

# 4. æ€§èƒ½æµ‹è¯•ï¼šæŸ¥è¯¢æœ€è¿‘ 30 å¤©è¡€ç³–å¹³å‡å€¼
echo ""
echo "âš¡ æ€§èƒ½æµ‹è¯• 2: æŸ¥è¯¢æœ€è¿‘ 30 å¤©è¡€ç³–å¹³å‡å€¼ï¼ˆæŒ‰æµ‹é‡æ—¶æœºåˆ†ç»„ï¼‰"
QUERY_2='from(bucket: "health_data")
  |> range(start: -30d, stop: now())
  |> filter(fn: (r) =>
      r._measurement == "blood_sugar" and
      r.user_id == "test_user_123" and
      r._field == "value"
  )
  |> group(columns: ["timing"])
  |> mean()
  |> rename(columns: {_value: "avg_value"})
  |> keep(columns: ["timing", "avg_value"])'

START=$(date +%s%N)
RESULT_2=$(curl -s -XPOST "${INFLUX_URL}/api/v2/query?org=${INFLUX_ORG}" \
  --header "Authorization: Token ${INFLUX_TOKEN}" \
  --header "Content-Type: application/vnd.flux" \
  --data "${QUERY_2}")
END=$(date +%s%N)
ELAPSED_MS=$(( (END - START) / 1000000 ))

echo "   å“åº”æ—¶é—´: ${ELAPSED_MS}ms"
if [ $ELAPSED_MS -lt 100 ]; then
  echo "   âœ… æ€§èƒ½è¾¾æ ‡ï¼ˆ< 100msï¼‰"
else
  echo "   âš ï¸  æ€§èƒ½æœªè¾¾æ ‡ï¼ˆç›®æ ‡ < 100msï¼‰"
fi

# 5. æ€§èƒ½æµ‹è¯•ï¼šæŸ¥è¯¢æŒ‡å®šæ—¶é—´èŒƒå›´çš„è¡€å‹ç»Ÿè®¡
echo ""
echo "âš¡ æ€§èƒ½æµ‹è¯• 3: æŸ¥è¯¢æŒ‡å®šæ—¶é—´èŒƒå›´çš„è¡€å‹ç»Ÿè®¡ï¼ˆèšåˆæŸ¥è¯¢ï¼‰"
START_TIME=$(date -u -d "30 days ago" --iso-8601=seconds)
STOP_TIME=$(date -u --iso-8601=seconds)

QUERY_3="from(bucket: \"health_data\")
  |> range(start: ${START_TIME}, stop: ${STOP_TIME})
  |> filter(fn: (r) =>
      r._measurement == \"blood_pressure\" and
      r.user_id == \"test_user_123\"
  )
  |> group(columns: [\"_field\"])
  |> reduce(
      fn: (r, accumulator) => ({
          _field: r._field,
          mean: accumulator.mean + r._value,
          max: if r._value > accumulator.max then r._value else accumulator.max,
          min: if r._value < accumulator.min then r._value else accumulator.min,
          count: accumulator.count + 1.0
      }),
      identity: {_field: \"\", mean: 0.0, max: 0.0, min: 999.0, count: 0.0}
  )
  |> map(fn: (r) => ({
      r with
      mean: r.mean / r.count
  }))
  |> keep(columns: [\"_field\", \"mean\", \"max\", \"min\", \"count\"])"

START=$(date +%s%N)
RESULT_3=$(curl -s -XPOST "${INFLUX_URL}/api/v2/query?org=${INFLUX_ORG}" \
  --header "Authorization: Token ${INFLUX_TOKEN}" \
  --header "Content-Type: application/vnd.flux" \
  --data "${QUERY_3}")
END=$(date +%s%N)
ELAPSED_MS=$(( (END - START) / 1000000 ))

echo "   å“åº”æ—¶é—´: ${ELAPSED_MS}ms"
if [ $ELAPSED_MS -lt 100 ]; then
  echo "   âœ… æ€§èƒ½è¾¾æ ‡ï¼ˆ< 100msï¼‰"
else
  echo "   âš ï¸  æ€§èƒ½æœªè¾¾æ ‡ï¼ˆç›®æ ‡ < 100msï¼‰"
fi

# 6. æ¸…ç†æµ‹è¯•æ•°æ®
echo ""
echo "ğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®..."
DELETE_PREDICATE="user_id=\"${USER_ID}\""
START_DELETE=$(date -u -d "60 days ago" --iso-8601=seconds)
STOP_DELETE=$(date -u --iso-8601=seconds)

curl -s -XPOST "${INFLUX_URL}/api/v2/delete?org=${INFLUX_ORG}&bucket=${INFLUX_BUCKET}" \
  --header "Authorization: Token ${INFLUX_TOKEN}" \
  --header "Content-Type: application/json" \
  --data "{\"start\":\"${START_DELETE}\",\"stop\":\"${STOP_DELETE}\",\"predicate\":\"${DELETE_PREDICATE}\"}" > /dev/null

echo "âœ… æµ‹è¯•æ•°æ®å·²æ¸…ç†"

echo ""
echo "======================================"
echo "æ€§èƒ½æµ‹è¯•å®Œæˆ"
echo "======================================"
